{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0deddeda-d0e3-4949-9c5a-5fc06297db5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba==0.53 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f54c86bf-3ef5-419b-b21a-c430964622bf/lib/python3.9/site-packages (0.53.0)\r\nRequirement already satisfied: numpy>=1.15 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (from numba==0.53) (1.22.4)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba==0.53) (58.0.4)\r\nRequirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f54c86bf-3ef5-419b-b21a-c430964622bf/lib/python3.9/site-packages (from numba==0.53) (0.36.0)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-f54c86bf-3ef5-419b-b21a-c430964622bf/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numba==0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba36354e-833f-41bf-b1ca-2796e63444b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from mlflow.tracking import MlflowClient\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima.stationarity import ADFTest\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from matplotlib.pyplot import figure\n",
    "from mlflow import pyfunc\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import mlflow\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "import pmdarima as pm\n",
    "import joblib\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from scipy import stats\n",
    "from pandas import Series\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# From Spiros\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#import seaborn as sns\n",
    "import pickle\n",
    "#import glob\n",
    "\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import sample\n",
    "import random\n",
    "from numpy.random import uniform\n",
    "from math import isnan\n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from kshape.core import kshape\n",
    "from tslearn.clustering import KShape, TimeSeriesKMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from numpy import array, newaxis\n",
    "from numpy import hstack\n",
    "from math import sqrt \n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Use Keras with Python 3.5\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, GRU, Input, Concatenate, Reshape, Lambda, Bidirectional\n",
    "# from keras.layers.normalization import BatchNormalization # tf.keras.layers.BatchNormalization()\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import load_model\n",
    "# from tensorflow import set_random_seed # set_random_seed(x) --> tensorflow.random.set_seed(x)\n",
    "import joblib # from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import shap\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "#from scipy import stats\n",
    "\n",
    "#-------------------------- for time series clustering ----------------------------\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#-------------------------- for non time series clustering ----------------------------\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "#---------------------------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# We will use the following indicators for evaluating the modelâ€™s performance\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "#-------------------------- for Seep learning (RNN / MLP) ----------------------------\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4e5330-4dfc-448e-894d-de68259c1780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"#===================================================================================================\"\"\"\n",
    "\"\"\"#============================================== Functions ==========================================\"\"\"\n",
    "\"\"\"#===================================================================================================\"\"\"\n",
    "def import_json(json_path):\n",
    "    df = pd.read_json(json_path)\n",
    "    normalized_data = pd.json_normalize(df.iloc[0,0])\n",
    "    normalized_data['time'] = pd.to_datetime(normalized_data['time'], utc=True)\n",
    "    normalized_data = normalized_data.set_index('time')\n",
    "    return normalized_data\n",
    "\n",
    "#================== Evaluating the model ================\n",
    "#Defining MAPE function\n",
    "def MAPE(Y_actual,Y_Predicted):\n",
    "    #mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    #return map\n",
    "    sum_ae = 0.0\n",
    "    mean_Yactual = np.mean(Y_actual)\n",
    "    for jj in range(len(Y_actual)):\n",
    "        if Y_actual[jj]==0:\n",
    "            sum_ae = sum_ae + np.abs((Y_actual[jj] - Y_Predicted[jj])/mean_Yactual)\n",
    "        else:\n",
    "            sum_ae = sum_ae + np.abs((Y_actual[jj] - Y_Predicted[jj])/Y_actual[jj])\n",
    "        mape = (sum_ae/len(Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "def add_calendar_components(data, calendar_components=None, drop_constant_columns=True):\n",
    "    \"\"\"\n",
    "    Add calendar components year, quarter, month, weekday, dayofyear, hour\n",
    "    to the input DataFrame.\n",
    "\n",
    "    :param data: input DataFrame with a DateTimeIndex and at least one column.\n",
    "    :param calendar_components: List of strings, specifying the calendar components you want to add in\n",
    "        [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"hour\"].\n",
    "    :param drop_constant_columns: If True, drops constant calendar components.\n",
    "    :return: new DataFrame with the added calendar components.\n",
    "    \"\"\"\n",
    "\n",
    "    if data.empty or not isinstance(data, pd.DataFrame) or not isinstance(data.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Input must be a non-empty pandas DataFrame with a DateTimeIndex.\")\n",
    "\n",
    "    default_components = [\"year\", \"quarter\", \"month\", \"weekday\", \"dayofyear\", \"hour\"]\n",
    "\n",
    "    if calendar_components is not None:\n",
    "        if not set(calendar_components).issubset(default_components):\n",
    "            raise ValueError(\"Argument 'calendar_components' must be a subset of \"\n",
    "                             \"['year', 'quarter', 'month', 'weekday', 'dayofyear', 'hour'].\")\n",
    "    else:\n",
    "        calendar_components = default_components\n",
    "\n",
    "    df_new = data.assign(**{\n",
    "        '{}'.format(component): getattr(data.index, component)\n",
    "        for component in calendar_components\n",
    "    })\n",
    "\n",
    "    if drop_constant_columns:\n",
    "        df_new = df_new.loc[:, (df_new[calendar_components] != df_new.iloc[0]).any()]\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801ba681-b019-4c81-8912-b183eee503dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ho: It is non stationary\n",
    "#H1: It is stationary\n",
    "\n",
    "def adfuller_test(df):\n",
    "    result=adfuller(df)\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "    for value,label in zip(result,labels):\n",
    "        print(label+' : '+str(value) )\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c39905-8c21-4c5d-ad6f-06830397e922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_mlflow(model, name, exp_id):\n",
    "\n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "\n",
    "        mlflow.autolog(log_models=True) \n",
    "        mlflow.log_param(\"order\", my_score )\n",
    "        mlflow.log_param(\"seasonal_order\", my_sea_score )\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2\", r2)\n",
    "        mlflow.log_artifact(\"sarimax.png\")\n",
    "        run_id = run.info.run_id\n",
    "        path = \"dbfs:/databricks/mlflow-tracking/{exp_id}/{run_id}\".format(exp_id = exp_id, run_id = run_id)\n",
    "        mlflow.statsmodels.save_model(model, path)\n",
    "        print(\"Run ID: \" + run_id)\n",
    "        print(\"run completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69d74c9-824d-46ff-8293-4e891440c7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation metrics for predictions.\n",
    ":parameter\n",
    "    :param dtf: DataFrame with columns raw values, fitted training  \n",
    "                 values, predicted test values\n",
    ":return\n",
    "    dataframe with raw ts and forecast\n",
    "'''\n",
    "def evaluate_forecast(dtf, title, plot=True):\n",
    "    try:\n",
    "        ## residuals\n",
    "        dtf[\"error\"] = dtf[\"ts\"] - dtf[\"forecast\"]\n",
    "        dtf[\"error_pct\"] = dtf[\"error\"] / dtf[\"ts\"]\n",
    "        \n",
    "        ## kpi\n",
    "        error_mean = dtf[\"error\"].mean()\n",
    "        error_std = dtf[\"error\"].std()\n",
    "        #mape = dtf[\"error_pct\"].apply(lambda x: np.abs(x)).mean()\n",
    "        global mae, mse, rmse, r2\n",
    "        mae = dtf[\"error\"].apply(lambda x: np.abs(x)).mean()\n",
    "        mse = dtf[\"error\"].apply(lambda x: x**2).mean()\n",
    "        rmse = np.sqrt(mse)  #root mean squared error\n",
    "        if np.mean(dtf[\"ts\"]) in dtf[\"ts\"].values:   \n",
    "          r2= 1-(sum(dtf[\"error\"]**2)/sum((dtf[\"ts\"]-(np.mean(dtf[\"ts\"] - 0.01))**2)))\n",
    "        else: \n",
    "          r2= 1-(sum(dtf[\"error\"]**2)/sum((dtf[\"ts\"]-np.mean(dtf[\"ts\"]))**2))\n",
    "        \n",
    "        ## intervals\n",
    "        dtf[\"pred_int_low\"] = dtf[\"forecast\"] - 1.96*error_std\n",
    "        dtf[\"pred_int_up\"] = dtf[\"forecast\"] + 1.96*error_std\n",
    "        \n",
    "        ## plot\n",
    "        if plot==True:\n",
    "            dtf = dtf.reset_index()\n",
    "            dtf['time'] = pd.to_datetime(dtf['time'])\n",
    "            dtf = dtf.set_index('time')\n",
    "            dtf = dtf.resample('1H').mean().fillna(0) \n",
    "            dtf[['ts','forecast']].plot(figsize=(12,8))\n",
    "            plt.xlabel('Date') \n",
    "            plt.ylabel('Consomation (Wh)')  \n",
    "            plt.title(\"Power value\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"sarimax.png\")\n",
    "           \n",
    "        print(\"Test --> Error mean:\", np.round(error_mean), \" | r2:\", np.round(r2,2),\n",
    "                  \" | mae:\",np.round(mae), \" | rmse:\",np.round(rmse))\n",
    "        \n",
    "        return dtf, mae, rmse, r2\n",
    "      \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"--- got error ---\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426606ef-34cd-4a56-9732-c72603397427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_stationarity_acf_pacf(data, sample, max_lag):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function tests the stationarity and plot the autocorrelation\n",
    "    and partial autocorrelation of the time series.\n",
    "    Test stationarity by:\n",
    "    - running Augmented Dickey-Fuller test with 95%\n",
    "    In statistics, the Dickeyâ€“Fuller test tests the null hypothesis\n",
    "    that a unit root is present in an autoregressive model.\n",
    "    The alternative hypothesis is different depending\n",
    "    on which version of the test is used,\n",
    "    but is usually stationarity or trend-stationarity.\n",
    "    - plotting mean and variance of a sample from data\n",
    "    - plotting autocorrelation and partial autocorrelation\n",
    "\n",
    "    p-value > 0.05: Fail to reject the null hypothesis (H0),\n",
    "    the data has a unit root and is non-stationary.\n",
    "    p-value <= 0.05: Reject the null hypothesis (H0),\n",
    "    the data does not have a unit root and is stationary.\n",
    "    This function is used to verify stationarity\n",
    "    so that suitable forecasting methods can be applied.\n",
    "\n",
    "    Partial autocorrelation is a summary of the relationship\n",
    "    between an observation in a time series with observations\n",
    "    at prior time steps with the relationships of intervening\n",
    "    observations removed. The partial autocorrelation at lag k\n",
    "    is the correlation that results after removing the effect\n",
    "    of any correlations due to the terms at shorter lags.\n",
    "\n",
    "    The autocorrelation for an observation and an observation\n",
    "    at a prior time step comprises both the direct correlation\n",
    "    and indirect correlations. These indirect correlations are\n",
    "    a linear function of the correlation of the observation,\n",
    "    with observations at intervening time steps.\n",
    "\n",
    "    These correlations are used to define the parameters\n",
    "    of the forecasting methods (lag).\n",
    "\n",
    "    :parameter\n",
    "     :param data: timeSeries for which the stationarity as to be evaluated.\n",
    "     :param sample: Sample (float) of the data that will be evaluated.\n",
    "     :param max_lag: Maximum number of lag which included in test.\n",
    "                    The default value is 12*(nobs/100)^{1/4}.\n",
    "    :return:\n",
    "      plot of the mean and variance of the sample with the p-value\n",
    "      and plot of the autocorrelation and partial autocorrelation.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data.empty:\n",
    "        raise ValueError(\"Input series must be not empty.\")\n",
    "\n",
    "    elif 0.0 > sample or sample > 1.0:\n",
    "        raise ValueError(\"Sample value should be between 0 and 1\")\n",
    "\n",
    "    for col in data:\n",
    "      ts_ax = plt.subplot2grid(shape=(2, 2), loc=(0, 0), colspan=2)\n",
    "      pacf_ax = plt.subplot2grid(shape=(2, 2), loc=(1, 0))\n",
    "      acf_ax = plt.subplot2grid(shape=(2, 2), loc=(1, 1))\n",
    "      dtf_ts = data[col].to_frame(name=\"ts\")\n",
    "      sample_size = int(len(data) * sample)\n",
    "      dtf_ts[\"mean\"] = dtf_ts[\"ts\"].head(sample_size).mean()\n",
    "      dtf_ts[\"lower\"] = dtf_ts[\"ts\"].head(sample_size).mean() + dtf_ts[\"ts\"].head(sample_size).std()\n",
    "      dtf_ts[\"upper\"] = dtf_ts[\"ts\"].head(sample_size).mean() - dtf_ts[\"ts\"].head(sample_size).std()\n",
    "      dtf_ts[\"ts\"].plot(ax=ts_ax, color=\"black\", legend=False)\n",
    "      dtf_ts[\"mean\"].plot(\n",
    "          ax=ts_ax, legend=False, color=\"red\",\n",
    "          linestyle=\"--\", linewidth=0.7)\n",
    "      ts_ax.fill_between(\n",
    "          x=dtf_ts.index, y1=dtf_ts['lower'],\n",
    "          y2=dtf_ts['upper'], color='lightskyblue', alpha=0.4)\n",
    "      dtf_ts[\"mean\"].head(sample_size).plot(\n",
    "          ax=ts_ax, legend=False,\n",
    "          color=\"red\", linewidth=0.9)\n",
    "      ts_ax.fill_between(\n",
    "          x=dtf_ts.index, y1=dtf_ts['lower'],\n",
    "          y2=dtf_ts['upper'], color='lightskyblue', alpha=0.4)\n",
    "      dtf_ts[\"mean\"].head(sample_size).plot(\n",
    "          ax=ts_ax, legend=False,\n",
    "          color=\"red\", linewidth=0.9)\n",
    "      ts_ax.fill_between(\n",
    "          x=dtf_ts.head(sample_size).index,\n",
    "          y1=dtf_ts['lower'].head(sample_size),\n",
    "          y2=dtf_ts['upper'].head(sample_size), color='lightskyblue')\n",
    "      adfuller_test = sm.tsa.stattools.adfuller(\n",
    "          data, maxlag=max_lag, autolag=\"AIC\")\n",
    "      adf, p, critical_value = adfuller_test[0], adfuller_test[1], adfuller_test[4][\"5%\"]\n",
    "      p = round(p, 3)\n",
    "      conclusion = \"Stationary\" if p < 0.05 else \"Non-Stationary\"\n",
    "      ts_ax.set_title(\n",
    "          'Dickey-Fuller Test 95%: ' + conclusion +\n",
    "          '(p value: ' + str(p) + ')')\n",
    "\n",
    "      # pacf (for AR) and acf (for MA)\n",
    "      smt.graphics.plot_pacf(\n",
    "          data, lags=30, ax=pacf_ax,\n",
    "          title=\"Partial Autocorrelation (for AR component)\")\n",
    "      smt.graphics.plot_acf(\n",
    "          data, lags=30, ax=acf_ax,\n",
    "          title=\"Autocorrelation (for MA component)\")\n",
    "      plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedb3649-db6e-4e25-aa68-fdd1b58bcad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_sarimax(ts_train, ts_test, exog_test, p, model):\n",
    "    \"\"\"\n",
    "    The function uses the model from the fit_sarimax function\n",
    "    to make predictions for the future value.\n",
    "\n",
    "    :parameter\n",
    "      :param ts_train: timeSeries used to train the model.\n",
    "      :param ts_test: timeSeries used to test the model.\n",
    "      :param exog_test: timeSeries containing the exogeneous variables.\n",
    "      :param p: number of periods to be forcasted.\n",
    "      :param model: model from the fit_sarimax function.\n",
    "\n",
    "    :return\n",
    "      Dataframe containing the true values and the forecasted ones.\n",
    "    \"\"\"\n",
    "\n",
    "    if ts_train.empty:\n",
    "        raise ValueError(\"Train series must be not empty.\")\n",
    "    elif ts_test.empty:\n",
    "        raise ValueError(\"Test series must be not empty.\")\n",
    "    elif not isinstance(p, int):\n",
    "        raise ValueError(\"p must be an integer.\")\n",
    "\n",
    "    for col in range(ts_test.shape[1]):\n",
    "      dtf_test = ts_test.iloc[:p, col].to_frame(name=\"ts\")\n",
    "\n",
    "    if exog_test is None:\n",
    "        dtf_test[\"forecast\"] = model.predict(\n",
    "            start=len(ts_train) ,\n",
    "            end=len(ts_train) + len(ts_test[1:p]),\n",
    "            exog=exog_test)\n",
    "    else:\n",
    "        dtf_test[\"forecast\"] = model.predict(\n",
    "            start=len(ts_train) ,\n",
    "            end=len(ts_train) + len(ts_test[:p-1]),\n",
    "            exog=exog_test[:p])\n",
    "    dtf_test = dtf_test.round()\n",
    "    return dtf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ee596e-6228-4e8d-9082-489866d098e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fit_sarimax(ts_train, order, seasonal_order, exog_train=None):\n",
    "    \"\"\"\n",
    "    Fit SARIMAX (Seasonal ARIMA with External Regressors):\n",
    "    y[t+1] = (c + a0*y[t] + a1*y[t-1] +...+ ap*y[t-p]) + (e[t] +\n",
    "                  b1*e[t-1] + b2*e[t-2] +...+ bq*e[t-q]) + (B*X[t])\n",
    "    :parameter\n",
    "    :param ts_train: pandas timeseries\n",
    "    :param order: tuple - ARIMA(p,d,q) --> p: lag order (AR), d:\n",
    "    degree of differencing (to remove trend), q: order\n",
    "                        of moving average (MA).\n",
    "    :param seasonal_order: tuple - (P,D,Q,s) --> s: number of\n",
    "                        observations per seasonal (ex. 7 for weekly\n",
    "                        seasonality with daily data, 12 for yearly\n",
    "                        seasonality with monthly data).\n",
    "    :param exog_train: pandas dataframe or numpy array.\n",
    "\n",
    "    :return\n",
    "    Model and dtf with the fitted values\n",
    "    \"\"\"\n",
    "    # train\n",
    "    if ts_train.empty:\n",
    "        raise ValueError(\"Train series must be not empty.\")\n",
    "\n",
    "    model = smt.SARIMAX(\n",
    "        ts_train, order=order, seasonal_order=seasonal_order,\n",
    "        exog=exog_train, enforce_stationarity=False,\n",
    "        enforce_invertibility=False, freq='H')\n",
    "    model = model.fit()\n",
    "    for col in ts_train:\n",
    "        dtf_train = ts_train[col].to_frame(name=\"ts\")\n",
    "        dtf_train[\"model\"] = model.fittedvalues\n",
    "\n",
    "    return dtf_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b053db-b7f3-494c-9e73-bad205114e0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def param_tuning_sarimax(data, m, max_order, information_criterion='aic'):\n",
    "    \"\"\"\n",
    "    Automatically discover the optimal order for a SARIMAX model.\n",
    "    The function works by conducting differencing tests to determine\n",
    "    the order of differencing, d, and then fitting models within ranges\n",
    "    of defined start_p, max_p, start_q, max_q ranges.\n",
    "\n",
    "    If the seasonal optional is enabled(allowing SARIMAX over ARIMA),\n",
    "    it also seeks to identify the optimal P and Q hyperparameters\n",
    "    after conducting the Canova-Hansen\n",
    "    to determine the optimal order of seasonal differencing, D.\n",
    "\n",
    "    In order to find the best model, it optimizes for\n",
    "    a given information_criterion\n",
    "    and returns the ARIMA which minimizes the value.\n",
    "\n",
    "    :parameter\n",
    "      :param data: timeSeries used to fit the sarimax estimator.\n",
    "      :param m: refers to the number of periods in each season.\n",
    "                      For example, m is 4 for quarterly data,\n",
    "                      12 for monthly data, or 1 for annual data.\n",
    "      :param max_order: maximum value of p+q+P+Q.\n",
    "                      If p+q >= max_order, a model will not be\n",
    "                      fit with those parameters and will progress\n",
    "                      to the next combination. Default is 5.\n",
    "      :param information_criterion: used to select the best model.\n",
    "                    Possibilities are â€˜aicâ€™, â€˜bicâ€™, â€˜hqicâ€™, â€˜oobâ€™.\n",
    "                    Default is 'aic'.\n",
    "\n",
    "    :return\n",
    "      best_model: model with the optimal parameters\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        raise ValueError(\"Input series must be not empty.\")\n",
    "    elif not isinstance(m, int):\n",
    "        raise ValueError(\"m must be an integer.\")\n",
    "\n",
    "    best_model = pm.auto_arima(data, start_p=1, start_q=1, start_P=2, start_Q=1,\n",
    "                               test='adf',  # use adftest to find optimal 'd'\n",
    "                               max_p=max_order, max_q=max_order,  # maximum p and q\n",
    "                               max_P=max_order, max_D=1,  # maximum P and D and Q\n",
    "                               max_Q=max_order,\n",
    "                               m=m,  # frequency of series # m=12 Monthly m=24 Hourly\n",
    "                               d=None,  # let model determine 'd'\n",
    "                               seasonal=True,  \n",
    "                               trace=True,\n",
    "                               exogenous=None,\n",
    "                               error_action='ignore',\n",
    "                               suppress_warnings=True,\n",
    "                               stepwise=True)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce3dfae-f915-4133-b1e8-aa69a321e0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test(ts, test=0.1, plot=True, figsize=(15,5)):\n",
    "    ## define splitting point\n",
    "    if type(test) is float:\n",
    "        split = int(len(ts)*(1-test))\n",
    "        perc = test\n",
    "    elif type(test) is str:\n",
    "        split = ts.reset_index()[ \n",
    "                      ts.reset_index().iloc[:,0]==test].index[0]\n",
    "        perc = round(len(ts[split:])/len(ts), 2)\n",
    "    else:\n",
    "        split = test\n",
    "        perc = round(len(ts[split:])/len(ts), 2)\n",
    "    print(\"--- splitting at index: \", split, \"|\", \n",
    "          ts.index[split], \"| test size:\", perc, \" ---\")\n",
    "    \n",
    "    ## split ts\n",
    "    global ts_train\n",
    "    global ts_test\n",
    "    ts_train = ts.head(split)\n",
    "    ts_test = ts.tail(len(ts)-split)\n",
    "\n",
    "    if plot is True:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, \n",
    "                               sharey=True, figsize=figsize)\n",
    "        ts_train.plot(ax=ax[0], grid=True, title=\"Train\", \n",
    "                      color=\"black\")\n",
    "        ts_test.plot(ax=ax[1], grid=True, title=\"Test\", \n",
    "                     color=\"blue\")\n",
    "        ax[0].set(xlabel=None)\n",
    "        ax[1].set(xlabel=None)\n",
    "        plt.show()\n",
    "\n",
    "        return ts_train, ts_test"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
